{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtecftwy/utseus-dives/blob/main/nbs/dive_2_1_llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Diving into LLMs</h1>"
      ],
      "metadata": {
        "id": "E-kIXTbTiTvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from nb_guided_dive.llm import *\n",
        "except ModuleNotFoundError:\n",
        "    print('installing nb_guided_dive')\n",
        "    !pip install -Uqq git+https://github.com/vtecftwy/nb-guided-dive.git\n",
        "    print('nb_guided_dive installed')\n",
        "    from nb_guided_dive.llm import *\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "except ModuleNotFoundError as e:\n",
        "    print('Must install transformers and accelerate before running')\n",
        "    !pip -qq install transformers accelerate\n",
        "    print(('transformers and accelerate installed'))\n",
        "\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "Pl4zZr-iQCEU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_bert_results(res):\n",
        "    for i, item in enumerate(res):\n",
        "        masked = item['token_str']\n",
        "        mask_s = item['sequence'].find(masked)\n",
        "        mask_e = mask_s + len(masked)\n",
        "        str_md = f\"- \" + item['sequence'][:mask_s] + '... **' + item['sequence'][mask_s:mask_e] + '** ...' + item['sequence'][mask_e:] + f\"(*Prob: {item['score']:.3f}*)\"\n",
        "        display(Markdown(str_md))\n",
        "\n",
        "def print_gpt2_results(res):\n",
        "    display(Markdown(res[0]['generated_text']))\n",
        "\n",
        "def md(text):\n",
        "    display(Markdown(text))\n"
      ],
      "metadata": {
        "id": "XLMdKyBtH-FJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will explore several open source LLMs of various capabilities:"
      ],
      "metadata": {
        "id": "GfgoXIKBibqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_checkpoint = 'google-bert/bert-base-uncased'\n",
        "gpt2_checkpoint = 'openai-community/gpt2'\n",
        "qwen_checkpoint = 'Qwen/Qwen2.5-1.5B-Instruct'\n",
        "deepseek_checkpoint = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'"
      ],
      "metadata": {
        "id": "3hNbjRYrQCBb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Models (Encoder Only): BERT"
      ],
      "metadata": {
        "id": "fk2uWa5wiSf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_ppl = pipeline('fill-mask', model=bert_checkpoint);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzdAb08Ib5Xe",
        "outputId": "23bbc19a-80e6-4f98-d1d7-65f7d364fd1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "children_names(bert_ppl.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUo925wbcFrW",
        "outputId": "50d5a80d-d8aa-4bac-a652-926026bbf01b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 top-level modules:\n",
            " - bert\n",
            " - cls\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is trained to predict which token was masked in a sentence.\n",
        "\n",
        "The masked token to predict is represented by the special token `[MASK]` (id: 103)."
      ],
      "metadata": {
        "id": "E6W8Yujdi4zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi Andrej Karpathy, are you [MASK] by the recent evolution of large language models.\"\n",
        "res = bert_ppl([text])\n",
        "print_bert_results(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mY24OdhedZqX",
        "outputId": "8fc900f9-22f0-4ffb-86d5-2ff67d52dc39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hi andrej karpathy, are you ... **surprised** ... by the recent evolution of large language models.(*Prob: 0.232*)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hi andrej karpathy, are you ... **intrigued** ... by the recent evolution of large language models.(*Prob: 0.071*)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hi andrej karpathy, are you ... **bothered** ... by the recent evolution of large language models.(*Prob: 0.062*)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hi andrej karpathy, are you ... **impressed** ... by the recent evolution of large language models.(*Prob: 0.050*)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hi andrej karpathy, are you ... **puzzled** ... by the recent evolution of large language models.(*Prob: 0.041*)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore BERT"
      ],
      "metadata": {
        "id": "YkWt0akFkcp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.charlier-tang.com/guided-dive/resources/img/LLM-Inside.jpg\"/>"
      ],
      "metadata": {
        "id": "4NU3Qq9VkgzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Tokenizer"
      ],
      "metadata": {
        "id": "4ksl0bCOmxf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tok = bert_ppl.tokenizer"
      ],
      "metadata": {
        "id": "uf-LdmtXkcgF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we apply the tokenizer to our text, it splits it into tokens and convert each token into an integer"
      ],
      "metadata": {
        "id": "uDQpJaPConxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = bert_tok(text)\n",
        "pos_mask = token_ids['input_ids'].index(103)\n",
        "token_ids['input_ids']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW-EaHYZQB4l",
        "outputId": "2a992a20-dc60-48a3-b6d9-f8f979360475"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 7632,\n",
              " 7213,\n",
              " 3501,\n",
              " 10556,\n",
              " 14536,\n",
              " 17308,\n",
              " 1010,\n",
              " 2024,\n",
              " 2017,\n",
              " 103,\n",
              " 2011,\n",
              " 1996,\n",
              " 3522,\n",
              " 6622,\n",
              " 1997,\n",
              " 2312,\n",
              " 2653,\n",
              " 4275,\n",
              " 1012,\n",
              " 102]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the split in text format as well"
      ],
      "metadata": {
        "id": "aiCzhRXppHCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_txt = bert_tok.tokenize(text)\n",
        "tokens_txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noNkCrp_QB7h",
        "outputId": "9a9653fe-1e1f-4319-bdbd-318a6ca9406a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi',\n",
              " 'andre',\n",
              " '##j',\n",
              " 'ka',\n",
              " '##rp',\n",
              " '##athy',\n",
              " ',',\n",
              " 'are',\n",
              " 'you',\n",
              " '[MASK]',\n",
              " 'by',\n",
              " 'the',\n",
              " 'recent',\n",
              " 'evolution',\n",
              " 'of',\n",
              " 'large',\n",
              " 'language',\n",
              " 'models',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each model has its own tokenizer, defining a list of tokens (vocab) and the mapping between text form and numeric form"
      ],
      "metadata": {
        "id": "96jSzhofpTAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "md(f\"BERT has a vocabulary ot **{bert_tok.vocab_size:,d}** tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "EIbXnnLQSUl7",
        "outputId": "4595d337-d14f-4357-9161-9cf5180c914b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "BERT has a vocabulary ot **30,522** tokens."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can randomly pick a sample of this vocabulary (text token and numerical token)"
      ],
      "metadata": {
        "id": "9ZNsH_usvmxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "items = list(bert_tok.vocab.items())\n",
        "selection = sorted(random.sample(items, 20))\n",
        "pprint({id:token for id, token in selection})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRs0D79sShJr",
        "outputId": "44c4163d-5314-4ca4-f843-6910f3cc90c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'##bruck': 28985,\n",
            " '##ey': 3240,\n",
            " '##jun': 19792,\n",
            " '##ntly': 20630,\n",
            " '[unused541]': 546,\n",
            " '[unused588]': 593,\n",
            " 'avid': 18568,\n",
            " 'botanist': 17098,\n",
            " 'chopper': 28057,\n",
            " 'kept': 2921,\n",
            " 'manually': 21118,\n",
            " 'olympus': 26742,\n",
            " 'physician': 7522,\n",
            " 'registrar': 24580,\n",
            " 'section': 2930,\n",
            " 'sierra': 7838,\n",
            " 'spilled': 13439,\n",
            " 'subfamily': 10946,\n",
            " 'translations': 11913,\n",
            " 'wally': 18202}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Embeddings"
      ],
      "metadata": {
        "id": "CqgFdfGonBsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the tokenizer, each token is converted into a vector (embedding) capturing the semantic meaning of the token."
      ],
      "metadata": {
        "id": "iqAn_srdv2Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model, bert_cls = get_children(bert_ppl.model)\n",
        "embs, encoder = get_children(bert_model)"
      ],
      "metadata": {
        "id": "bbj2RhvbkcaF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to get to the embedding:\n",
        "- text to token_ids with the tokenizer\n",
        "- token_ids to embeddings (one vector per token) with the embedding layer\n"
      ],
      "metadata": {
        "id": "qRIq5GYFqUhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "md(f\"'*{text}*'\")\n",
        "encoded_input = bert_tok(text, return_tensors='pt')['input_ids'].to(bert_ppl.model.device)\n",
        "print(encoded_input)\n",
        "md(f\"Text encoded into **{encoded_input.shape[1]} tokens**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "LVIdh8sKkcTQ",
        "outputId": "2e86101d-8f53-4905-a4a1-952a8c4def3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "'*Hi Andrej Karpathy, are you [MASK] by the recent evolution of large language models.*'"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  7632,  7213,  3501, 10556, 14536, 17308,  1010,  2024,  2017,\n",
            "           103,  2011,  1996,  3522,  6622,  1997,  2312,  2653,  4275,  1012,\n",
            "           102]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Text encoded into **21 tokens**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embs(encoded_input)\n",
        "md(f\"Embedding layer outputs **{embeddings.shape[1]:,d} embedding vectors**. One per token\")\n",
        "md(f\"Each embedding vector has **{embeddings.shape[2]:,d}** values. The embedding space has {embeddings.shape[1]} dimensions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "WPbXHbHUkcP5",
        "outputId": "99ce1564-7ff0-41cc-92b5-a0b2abfd334e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Embedding layer outputs **21 embedding vectors**. One per token"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Each embedding vector has **768** values. The embedding space has 21 dimensions."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzeAbQvokcM2",
        "outputId": "6f700cec-ebd8-4371-cd9a-a806c2b84611"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
              "         [ 0.2743, -0.8239,  0.8169,  ..., -0.3081, -0.8084, -0.6725],\n",
              "         [ 0.2850,  0.1554, -0.1726,  ...,  0.8624,  0.9615,  0.5178],\n",
              "         ...,\n",
              "         [ 0.0755,  0.9946, -0.3133,  ..., -0.3525, -0.4831,  0.3692],\n",
              "         [-0.1950,  0.3005, -0.2888,  ...,  0.5752,  0.4682,  0.5196],\n",
              "         [-0.0907,  0.1288, -0.2127,  ..., -0.2871,  0.1035, -0.1262]]],\n",
              "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBg4WuEvkcJX",
        "outputId": "4d728833-4ca0-4fed-dfe6-064c0e590a49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 21, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Classifier"
      ],
      "metadata": {
        "id": "n0nfCNAK5oey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, LLM are essentially token classifiers:"
      ],
      "metadata": {
        "id": "oOoP9cZMw9QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.charlier-tang.com/guided-dive/resources/img/llm-as-token-classifier.jpg\"/>"
      ],
      "metadata": {
        "id": "W08t0s5quOXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "children_names(bert_ppl.model)\n",
        "print()\n",
        "children_names(bert_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcaTDpqIxGN7",
        "outputId": "6dda760b-b34c-4199-8d16-a0bbc0b0981d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 top-level modules:\n",
            " - bert\n",
            " - cls\n",
            "\n",
            "We have 2 top-level modules:\n",
            " - embeddings\n",
            " - encoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_cls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVSYpoA4xnUx",
        "outputId": "c76f4045-2e52-4323-fd4e-05cb2dc1b378"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertOnlyMLMHead(\n",
              "  (predictions): BertLMPredictionHead(\n",
              "    (transform): BertPredictionHeadTransform(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (transform_act_fn): GELUActivation()\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last layer of BERT takes the final embedding vector (768 values) and outputs \"probabilities\" of being the masked token for each of the tokens in the vocab (30,522)"
      ],
      "metadata": {
        "id": "NqzK8yJ8xsUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_probs = bert_ppl.model(encoded_input).logits[0, pos_mask, :]\n",
        "pred_probs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODoR55k-tJOw",
        "outputId": "f9bcc3fb-7b18-49e1-9151-7bf11ad3a697"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30522])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md(text)\n",
        "v,idx = torch.topk(pred_probs, k=20, dim=0)\n",
        "for pred in bert_tok.decode(idx).split(' '):\n",
        "    md(f\"- {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "3oIbvCPu2AjS",
        "outputId": "4efcefe5-e85b-4a2f-c9ce-eb1a7ae93ebf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Andrej Karpathy, are you [MASK] by the recent evolution of large language models."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- surprised"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- intrigued"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- bothered"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- impressed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- puzzled"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- confused"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- amused"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- concerned"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- shocked"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- disappointed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- disturbed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- influenced"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- fascinated"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- interested"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- excited"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- inspired"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- amazed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- startled"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- attracted"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- offended"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v,idx = torch.topk(pred_probs, k=40, dim=0, largest=False)\n",
        "for pred in bert_tok.decode(idx).split(' '):\n",
        "    md(f\"- {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "kBO6PC8DzmDx",
        "outputId": "261a3089-a120-4c5a-dac0-2fff0af7443f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- ##encyignort"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- shortestncylchpregramility"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- luxrialishlyrchllisildentlypictryittously"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- longest"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- mutuallyrgh"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- sac"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- titular"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- federallylgearmllarticknch"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- pillow"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- facto"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- hashndenhom"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- bark"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- lifetimerumlessly"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal - Autoregressive Model (Decoder only): GPT2"
      ],
      "metadata": {
        "id": "V68W4OctjIiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_ppl = pipeline('text-generation', model=gpt2_checkpoint);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgWqgUk6h6h_",
        "outputId": "43ed4a79-5629-4fbc-8d9e-9dcf2648551a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "children_names(gpt2_ppl.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMhACOdujZJf",
        "outputId": "0c902f79-037c-4129-a35a-493f3c149475"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 top-level modules:\n",
            " - transformer\n",
            " - lm_head\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hi Andrej Karpathy, what do you think of the recent evolution of large language models? I am absolutely '\n",
        "\n",
        "res = gpt2_ppl(text, max_new_tokens=30)\n",
        "print_gpt2_results(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "id": "3nAYeDH5jleO",
        "outputId": "34787b1d-7d49-4e13-ce53-55fdb7b3ec51"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hi Andrej Karpathy, what do you think of the recent evolution of large language models? I am absolutely icty-dazed because I was not expecting to see so many new research coming out about languages from different parts of the world. So I thought"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleanup Memory"
      ],
      "metadata": {
        "id": "Lpnx2hrW9sdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.cpu()\n",
        "gpt2_ppl.model.cpu()\n",
        "clean_up(bert_model, bert_tok, gpt2_ppl, encoded_input)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "p6R8Jc2t9vNA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuned LLM: DeepSeek R1"
      ],
      "metadata": {
        "id": "DjGXomTY5MVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_ppl = pipeline(\"text-generation\", model=deepseek_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICx40otwKVik",
        "outputId": "379f09ad-1093-48df-95b9-b7b8f8d2716a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a conversation prompt\n",
        "\n",
        "Fine tuned model expect structured prompts indicating a \"role\" and the \"content\"."
      ],
      "metadata": {
        "id": "p1JlHhA-CyaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain quantum entanglement as it you were a teacher\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]"
      ],
      "metadata": {
        "id": "xWHaZYksKYDk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate response"
      ],
      "metadata": {
        "id": "LMCVLBxZKXRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = deepseek_ppl(messages, max_new_tokens=800)\n",
        "md(f\"{len(response)} response of {len(response[0]['generated_text'][1]['content'])} characters\")"
      ],
      "metadata": {
        "id": "lNGOG1XNKYqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c14a72-4e88-4ab3-b8af-d7dbe7948dee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3919)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md(response[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "id": "-Zd2DDzzNEx2",
        "outputId": "c093d0ad-aafc-4e46-c541-6b94b5123b6e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, so I'm trying to understand quantum entanglement. I remember hearing the term in a science class, but I'm not really clear on what it means. Let me think about this step by step.\n\nFirst, I know that entanglement is something related to particles, probably light particles like photons. But wait, isn't that true for all particles, or is it a special kind? I think it's a phenomenon where particles become interconnected, so their properties are linked, even when they're far apart. But how exactly does that work?\n\nI remember something about particles having states. Like, maybe each particle can be in multiple states at the same time. For example, an electron could be in an up spin or down spin state. But if two particles are entangled, their spins are linked. So if one is up, the other must be down, and vice versa. That makes sense, but how does that actually happen?\n\nI think it's about the quantum state of the system. When two particles are entangled, their quantum states are correlated. So if I measure one, the other instantly gives a result based on that measurement. That seems spooky because it's instantaneous, not just due to distance. But how is this possible? Is it because of something like a wave function that's not just a simple probability distribution?\n\nI'm also thinking about the EPR paradox, which was about whether quantum entanglement was real. Einstein, Podolsky, and Rosen thought it was, but later experiments suggested it's more than that. Maybe it's about how the universe doesn't know which state the particles are in until you measure them. So the act of measurement affects the other particle, even if they're far apart.\n\nThere's this Bell's theorem thing too. I think it's about whether local hidden variables (which are like classical theories) could explain entanglement. But experiments show that no local hidden variables can explain the results, so entanglement must be a real quantum effect. That's pretty mind-blowing.\n\nBut wait, how does this apply in real life? I know that quantum computing uses entangled particles to process information faster. So if we can entangle particles, we can do things like teleporting one particle's state to another, which would be useful for secure communication. But I'm not sure how practical that is yet. Also, the faster-than-light communication is still a big mystery. We don't know if it's possible or how it would work.\n\nI'm also confused about the difference between quantum entanglement and classical correlations. I think entanglement is stronger than classical correlations because it's due to the quantum state being entangled, not just classical probabilities. So it's not just about correlations between two particles but something more fundamental about the system.\n\nAnother thing I'm trying to grasp is the concept of \"spooky action at a distance.\" Einstein associated this with the non-local nature of quantum mechanics, implying that particles could influence each other regardless of distance. But I'm not entirely sure if that's the same as entanglement or just a different term for the same phenomenon.\n\nI'm also wondering about the mathematical description of entanglement. The wave function of the system must be a superposition of states. When you measure one particle, the wave function collapses, and the other particle's state becomes determined. But how does this collapse work exactly? Is it deterministic, or is it probabilistic?\n\nI think it's deterministic in the sense that the wave function is a fixed state, and the measurement outcome is determined by the quantum state. But since we don't know the exact state until we measure, it's like we're just learning about it. So it's not like a hidden variable theory where you can predict with certainty, but rather a probabilistic reality where the outcome is determined by the quantum state, but we don't know what it is until we observe it.\n\nI'm also"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain quantum entanglement like if you were a teacher. Provide the answer in markdown format\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "response = deepseek_ppl(messages, max_new_tokens=800)\n",
        "md(f\"{len(response)} response of {len(response[0]['generated_text'][1]['content'])} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "Ch9QhSGXNvE_",
        "outputId": "d0d42b52-569d-4020-b14b-3f40cda26b89"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1 response of 3926 characters"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md(response[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "VcXIZ7GhN4Ft",
        "outputId": "b77943b9-1700-4b68-c05f-f73c0e1c0573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, so I need to explain quantum entanglement like a teacher. Hmm, where do I start? I remember learning about entanglement in my science class, but I'm a bit fuzzy on the details. Let me try to piece this together.\n\nFirst, I think entanglement has something to do with particles being connected in a way that the state of one instantly influences the state of another, no matter the distance between them. But how exactly does that work? I remember something about particles being in multiple states at once, like superposition. So maybe when you measure one, it affects the other.\n\nWait, so particles like electrons can be in a superposition of states, right? That means they can be in two places at once. But how does that lead to them being connected? I think it's because the act of measuring one affects the other, but I'm not entirely sure how that happens.\n\nI remember something about the EPR paradox, which involves Einstein, Podolsky, and Rosen. They proposed that entanglement is a real phenomenon that Einstein didn't accept because he thought it contradicts relativity. But now, experiments have shown that entanglement does happen, so maybe it's more about the statistical correlation rather than violating relativity.\n\nSo, quantum entanglement is when two particles share a common state. When you measure one, the other's state collapses instantly, even if they're far apart. But how does this work with probabilities? I think it's about the correlation between the probabilities of the outcomes. For example, if you measure one particle and get a certain result, the other has a high probability of matching that result.\n\nWait, but how does this work probabilistically? If I measure one particle and get heads, does the other always get heads too? Or is there a chance it could get tails? I think it's the latter. So, there's a probability distribution between them, and that's what's being referred to as entanglement.\n\nI'm also a bit confused about the difference between quantum entanglement and classical correlation. I think quantum entanglement is stronger because it's based on the principles of quantum mechanics, whereas classical correlation is more about classical physics, like the correlation in everyday phenomena.\n\nAnother thing I'm not sure about is the role of the observer. Does the act of measuring affect the state of the particle? I think yes. So, the observer collapses the wavefunction, and that affects the other particle. But I'm not entirely clear on how that leads to the connection.\n\nI also recall that entanglement is a key part of quantum computing and quantum communication. So, it's not just a theoretical concept but has practical applications. But why is it important? Maybe it's because it allows for faster and more secure communication, which is useful in fields like cryptography.\n\nWait, but I'm still not clear on the mathematical representation. How do we describe entangled states? I think it's using vectors and matrices, something like the Bell states. Each Bell state represents a possible entangled state between two qubits. But I'm not exactly sure how these states are constructed or what they signify.\n\nAlso, I'm trying to remember if there are any real-world applications beyond quantum computing. I think it's used in quantum cryptography for secure communication, like quantum key distribution, which relies on the principles of entanglement. But I'm not sure how that works exactly.\n\nI'm getting a bit overwhelmed with all these concepts. Maybe I should break it down into simpler parts. Start with the basics of quantum mechanics, then move on to entanglement, and then discuss its implications and applications.\n\nSo, to summarize my thoughts: Entanglement is when quantum particles share a common state, and the measurement of one affects the other instantaneously. This leads to correlations in their states, which can be stronger than classical correlations. It's a phenomenon supported by experiments and has applications"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_ppl.model.cpu()\n",
        "clean_up(deepseek_ppl)\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "9wlgfcNpEeQe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuned LLS: Alibaba Qwen"
      ],
      "metadata": {
        "id": "VDj7aRI66MLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_ppl = pipeline(\"text-generation\",model=qwen_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RHVGa9pQbL3",
        "outputId": "6279f862-a0d3-4bc9-9121-26f2e1384a51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create conversation prompt\n",
        "messages = [{\"role\": \"user\", \"content\": \"Explain quantum entanglement to me as if you were a teacher. Format reply in markdown\"}]\n",
        "\n",
        "# Generate response\n",
        "response = qwen_ppl(messages, max_new_tokens=800)\n",
        "print(response[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HH57CVRPfo",
        "outputId": "5a6c934e-78ee-42bf-98c7-1ce094c8a6a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': 'Explain quantum entanglement to me as if you were a teacher. Format reply in markdown'}, {'role': 'assistant', 'content': '# Quantum Entanglement Explained\\n\\n## Introduction\\nQuantum entanglement is a phenomenon that occurs when pairs or groups of particles interact so strongly that the state of one particle cannot be described independently of the others. This means that the state of each particle depends on the states of all other particles in the system.\\n\\n## Basic Concepts\\n1. **Particles**: In quantum mechanics, every particle has an associated wave function, which describes its possible positions and momenta.\\n2. **Entangled Particles**: When two or more particles become entangled, their combined wave functions describe not just the individual states but also the correlations between them.\\n\\n## The Principle of Non-Locality\\nThe most famous example of quantum entanglement is Einstein\\'s \"spooky action at a distance.\" According to special relativity, information can\\'t travel faster than light; thus, it seems paradoxical for particles connected through entanglement to instantaneously influence each other over large distances.\\n\\n3. **Bell\\'s Theorem**: John Stewart Bell formulated a theorem that showed how measurements on entangled particles must always give correlated results regardless of the distance separating them, even when measured simultaneously. This directly contradicts classical physics\\' view of causality and locality.\\n\\n4. **EPR Paradox (Einstein-Podolsky-Rosen)**: A thought experiment involving entangled particles where measurements on one particle seem to instantly affect the outcome of another, no matter how far apart they are.\\n\\n5. **No-Signaling Condition**: Another way to express this non-locality is through a condition known as the no-signaling principle, which requires that the measurement outcomes of entangled particles should not depend on whether signals have been sent between them before making a measurement.\\n\\n6. **Uncertainty Principle**: Unlike classical systems, entangled particles exhibit phenomena like superposition and interference, allowing us to perform tasks impossible with classical bits. This is because these properties donâ€™t commute, meaning that measuring one part affects the probability amplitudes of the other parts.\\n\\n7. **Quantum Teleportation**: An early demonstration of entanglement using photons was used to transmit quantum information from one location to another without physically sending any particles themselves.\\n\\n8. **Quantum Information Processing**: Today, entanglement plays a crucial role in many advanced technologies including quantum computing, cryptography, and teleportation, providing unprecedented computational power beyond what is achievable with classical computers.\\n\\n9. **Quantum Cryptography**: Uses entangled particles to create unbreakable codes based on the principles of quantum mechanics, ensuring secure communication channels.\\n\\n10. **Quantum Key Distribution (QKD)**: A method of transmitting cryptographic keys securely using quantum mechanics, enabling encrypted communications protected against eavesdropping.\\n\\n## Conclusion\\nQuantum entanglement represents a fundamental aspect of quantum mechanics that challenges our understanding of space-time and causality. It opens up new possibilities for technology and scientific research, pushing the boundaries of what we know about the universe at the smallest scales.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "5ZBqbYbjRayH",
        "outputId": "2bcfedd1-5b56-4fc3-b048-de8f816ca90f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Quantum Entanglement Explained\n\n## Introduction\nQuantum entanglement is a phenomenon that occurs when pairs or groups of particles interact so strongly that the state of one particle cannot be described independently of the others. This means that the state of each particle depends on the states of all other particles in the system.\n\n## Basic Concepts\n1. **Particles**: In quantum mechanics, every particle has an associated wave function, which describes its possible positions and momenta.\n2. **Entangled Particles**: When two or more particles become entangled, their combined wave functions describe not just the individual states but also the correlations between them.\n\n## The Principle of Non-Locality\nThe most famous example of quantum entanglement is Einstein's \"spooky action at a distance.\" According to special relativity, information can't travel faster than light; thus, it seems paradoxical for particles connected through entanglement to instantaneously influence each other over large distances.\n\n3. **Bell's Theorem**: John Stewart Bell formulated a theorem that showed how measurements on entangled particles must always give correlated results regardless of the distance separating them, even when measured simultaneously. This directly contradicts classical physics' view of causality and locality.\n\n4. **EPR Paradox (Einstein-Podolsky-Rosen)**: A thought experiment involving entangled particles where measurements on one particle seem to instantly affect the outcome of another, no matter how far apart they are.\n\n5. **No-Signaling Condition**: Another way to express this non-locality is through a condition known as the no-signaling principle, which requires that the measurement outcomes of entangled particles should not depend on whether signals have been sent between them before making a measurement.\n\n6. **Uncertainty Principle**: Unlike classical systems, entangled particles exhibit phenomena like superposition and interference, allowing us to perform tasks impossible with classical bits. This is because these properties donâ€™t commute, meaning that measuring one part affects the probability amplitudes of the other parts.\n\n7. **Quantum Teleportation**: An early demonstration of entanglement using photons was used to transmit quantum information from one location to another without physically sending any particles themselves.\n\n8. **Quantum Information Processing**: Today, entanglement plays a crucial role in many advanced technologies including quantum computing, cryptography, and teleportation, providing unprecedented computational power beyond what is achievable with classical computers.\n\n9. **Quantum Cryptography**: Uses entangled particles to create unbreakable codes based on the principles of quantum mechanics, ensuring secure communication channels.\n\n10. **Quantum Key Distribution (QKD)**: A method of transmitting cryptographic keys securely using quantum mechanics, enabling encrypted communications protected against eavesdropping.\n\n## Conclusion\nQuantum entanglement represents a fundamental aspect of quantum mechanics that challenges our understanding of space-time and causality. It opens up new possibilities for technology and scientific research, pushing the boundaries of what we know about the universe at the smallest scales."
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}